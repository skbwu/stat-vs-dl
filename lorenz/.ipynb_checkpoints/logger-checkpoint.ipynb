{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d3b5016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, pickle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a11b4f",
   "metadata": {},
   "source": [
    "# In-Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d9208a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df76754bea7a4c079aeaf4d3717a8816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 09:47:13.746404: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-20 09:47:13.782381: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-20 09:47:13.786751: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-20 09:47:13.797458: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-20 09:47:13.813834: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-20 09:47:13.818635: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-20 09:47:13.835862: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-20 09:47:25.883284: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944c3e330d5b4c6ebbad747f8bfd6e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start insample logs dataframe? \"err\" = error (unscaled, no abs. value)\n",
    "cols = [\"model\", \"lmbda\", \"rho\", \"alpha\", \"dobs\", \"seed\", \n",
    "        \"betahat\", \"betahat_LB\", \"betahat_UB\", \"betahat_err\",\n",
    "        \"rhohat\", \"rhohat_LB\", \"rhohat_UB\", \"rhohat_err\",\n",
    "        \"sigmahat\", \"sigmahat_LB\", \"sigmahat_UB\", \"sigmahat_err\",\n",
    "        \"X_rmse\", \"Y_rmse\", \"Z_rmse\"]\n",
    "\n",
    "# instantiate our dataframe\n",
    "insample_logs, t_max = pd.DataFrame(data=None, columns=cols), 8.0\n",
    "\n",
    "########\n",
    "\n",
    "# in-sample logs first (MAGI) - theta pt estim, interval estim, covgs, theta-AEs, {X,Y,Z}-RMSEs\n",
    "model = \"magi\"\n",
    "\n",
    "# what are our relevant magi files?\n",
    "fnames = sorted([f for f in os.listdir(\"results/magi\") if \"rho\" in f])\n",
    "\n",
    "# go thru each file + load in our .pickle, analyze the results\n",
    "for fname in tqdm(fnames):\n",
    "    \n",
    "    # create our row for the dataframe\n",
    "    rho, alpha, dobs, _, seed = [float(s.split(\"=\")[1]) for s in fname.replace(\".pickle\", \"\").split(\"_\")]\n",
    "    dobs, seed = int(dobs), int(seed)\n",
    "    row = [model, np.nan, rho, alpha, dobs, seed]\n",
    "    \n",
    "    # open up the pickle file\n",
    "    with open(f\"results/magi/{fname}\", \"rb\") as file:\n",
    "        results = pickle.load(file)\n",
    "        \n",
    "    # get our param estimates\n",
    "    betahat, rhohat, sigmahat = results[\"thetas_samps\"].mean(axis=0)\n",
    "    betahat_LB, rhohat_LB, sigmahat_LB = np.quantile(a=results[\"thetas_samps\"], q=0.025, axis=0)\n",
    "    betahat_UB, rhohat_UB, sigmahat_UB = np.quantile(a=results[\"thetas_samps\"], q=0.975, axis=0)\n",
    "    \n",
    "    # compute our errors\n",
    "    betahat_err, rhohat_err, sigmahat_err = betahat - (8/3), rhohat - rho, sigmahat - 10.0\n",
    "    \n",
    "    # get our ground truth\n",
    "    truth = pd.read_csv(f\"data/LORENZ_rho={rho}_alpha={alpha}_seed={seed}.csv\").query(f\"t <= {t_max}\")\n",
    "    truth = truth.iloc[::int((truth.index.shape[0] - 1) / (dobs * t_max))]\\\n",
    "    [[\"t\", \"X_true\", \"Y_true\", \"Z_true\"]].set_index(\"t\")\n",
    "    truth.index = np.round(truth.index, 5)\n",
    "    \n",
    "    # get our predictions for ts_obs ONLY!\n",
    "    preds = pd.DataFrame(data=np.hstack([results[\"I\"], results[\"X_samps\"].mean(axis=0)]), \n",
    "                         columns=[\"t\", \"X\", \"Y\", \"Z\"]).set_index(\"t\")\n",
    "    preds.index = np.round(preds.index, 5); preds = preds.loc[truth.index]\n",
    "    \n",
    "    # compute our RMSE on the ts_obs datapoints only!\n",
    "    X_rmse, Y_rmse, Z_rmse = np.sqrt(((preds.values - truth.values) ** 2).mean(axis=0))\n",
    "    \n",
    "    # assemble our row + add to our dataframe\n",
    "    row += [betahat, betahat_LB, betahat_UB, betahat_err, \n",
    "            rhohat, rhohat_LB, rhohat_UB, rhohat_err,\n",
    "            sigmahat, sigmahat_LB, sigmahat_UB, sigmahat_err,\n",
    "            X_rmse, Y_rmse, Z_rmse]\n",
    "    insample_logs.loc[len(insample_logs.index)] = row\n",
    "    \n",
    "########\n",
    "\n",
    "# repeat the process for PINN\n",
    "model = \"pinn\"\n",
    "\n",
    "# what are our relevant PINN files?\n",
    "fnames = sorted([f for f in os.listdir(\"results/pinn\") if \"rho\" in f])\n",
    "\n",
    "# go thru each folder + load in our .csvs as called for\n",
    "for fname in tqdm(fnames):\n",
    "    \n",
    "    # create our row for the dataframe\n",
    "    lmbda, rho, alpha, dobs, seed = [float(s.split(\"=\")[1]) for s in fname.split(\"_\")]\n",
    "    dobs, seed = int(dobs), int(seed)\n",
    "    row = [model, lmbda, rho, alpha, dobs, seed]\n",
    "    \n",
    "    # get our param estimates and compute our errors\n",
    "    betahat, rhohat, sigmahat = pd.read_csv(f\"results/pinn/{fname}/theta_hats.csv\").values[0]\n",
    "    betahat_LB, rhohat_LB, sigmahat_LB = np.nan, np.nan, np.nan\n",
    "    betahat_UB, rhohat_UB, sigmahat_UB = np.nan, np.nan, np.nan\n",
    "    \n",
    "    # compute our errors\n",
    "    betahat_err, rhohat_err, sigmahat_err = betahat - (8/3), rhohat - rho, sigmahat - 10.0\n",
    "    \n",
    "    # get our ground truth\n",
    "    truth = pd.read_csv(f\"data/LORENZ_rho={rho}_alpha={alpha}_seed={seed}.csv\").query(f\"t <= {t_max}\")\n",
    "    truth = truth.iloc[::int((truth.index.shape[0] - 1) / (dobs * t_max))]\\\n",
    "    [[\"t\", \"X_true\", \"Y_true\", \"Z_true\"]].set_index(\"t\")\n",
    "    truth.index = np.round(truth.index, 5)\n",
    "    \n",
    "    # get our predictions for ts_obs ONLY!\n",
    "    preds = pd.read_csv(f\"results/pinn/{fname}/in-sample_preds.csv\").set_index(\"t\")\n",
    "    \n",
    "    # compute our RMSE on the ts_obs datapoints only!\n",
    "    X_rmse, Y_rmse, Z_rmse = np.sqrt(((preds.values - truth.values) ** 2).mean(axis=0))\n",
    "    \n",
    "    # assemble our row + add to our dataframe\n",
    "    row += [betahat, betahat_LB, betahat_UB, betahat_err, \n",
    "            rhohat, rhohat_LB, rhohat_UB, rhohat_err,\n",
    "            sigmahat, sigmahat_LB, sigmahat_UB, sigmahat_err,\n",
    "            X_rmse, Y_rmse, Z_rmse]\n",
    "    insample_logs.loc[len(insample_logs.index)] = row\n",
    "    \n",
    "# save our dataframe to .csv\n",
    "insample_logs.to_csv(\"logs/insample_logs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f0210c",
   "metadata": {},
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b66304b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d012afd4e1045508c4c0ff901132e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00df0718e846499683ee35ae2c593947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start our forecasting logs (let's go until TM=5.0, only compute on the out-of-sample!)\n",
    "cols = [\"model\", \"lmbda\", \"rho\", \"alpha\", \"dobs\", \"seed\", \"X_rmse\", \"Y_rmse\", \"Z_rmse\"]\n",
    "t_max, t_max_pred = 2.0, 5.0\n",
    "\n",
    "# create our dataframe to store our results\n",
    "forecast_logs = pd.DataFrame(data=None, columns=cols)\n",
    "\n",
    "########\n",
    "\n",
    "# let's start with MAGI first\n",
    "model = \"magi\"\n",
    "\n",
    "# what are our relevant magi files?\n",
    "fnames = sorted([f for f in os.listdir(\"results/magi_forecasting\") if \"rho\" in f])\n",
    "\n",
    "for fname in tqdm(fnames):\n",
    "    \n",
    "    # create our row for the dataframe\n",
    "    rho, alpha, dobs, _, seed = [float(s.split(\"=\")[1]) for s in fname.replace(\".pickle\", \"\").split(\"_\")]\n",
    "    dobs, seed = int(dobs), int(seed)\n",
    "    row = [model, np.nan, rho, alpha, dobs, seed]\n",
    "    \n",
    "    # open up the pickle file\n",
    "    with open(f\"results/magi_forecasting/{fname}\", \"rb\") as file:\n",
    "        results = pickle.load(file)\n",
    "    \n",
    "    # extract our predictions on the desired interval (t_max, t_max_pred]\n",
    "    preds = pd.DataFrame(\n",
    "    np.hstack([results[3][\"I\"], \n",
    "               results[3][\"X_samps\"].mean(axis=0)]), \n",
    "    columns=[\"t\", \"X\", \"Y\", \"Z\"]).query(f\"t > {t_max} and t <= {t_max_pred}\").set_index(\"t\")\n",
    "    preds.index = np.round(preds.index, 5)\n",
    "    \n",
    "    # get our truth\n",
    "    truth = pd.read_csv(f\"data/LORENZ_rho={rho}_alpha={alpha}_seed={seed}.csv\")\\\n",
    "    .query(f\"t > {t_max} and t <= {t_max_pred}\")[[\"t\", \"X_true\", \"Y_true\", \"Z_true\"]].set_index(\"t\")\n",
    "    truth.index = np.round(truth.index, 5); truth = truth.loc[preds.index]\n",
    "    \n",
    "    # compute our RMSEs on the forecasting regions only\n",
    "    X_rmse, Y_rmse, Z_rmse = np.sqrt(((truth.values - preds.values) ** 2).mean(axis=0))\n",
    "    \n",
    "    # add to our row, append to our dataframe\n",
    "    row += [X_rmse, Y_rmse, Z_rmse]\n",
    "    forecast_logs.loc[len(forecast_logs.index)] = row\n",
    "    \n",
    "########\n",
    "\n",
    "# proceed to PINN\n",
    "model = \"pinn\"\n",
    "\n",
    "# what are our relevant magi files?\n",
    "fnames = sorted([f for f in os.listdir(\"results/pinn_forecasting\") if \"rho\" in f])\n",
    "\n",
    "for fname in tqdm(fnames):\n",
    "    \n",
    "    # create our row for the dataframe\n",
    "    lmbda, rho, alpha, dobs, seed = [float(s.split(\"=\")[1]) for s in fname.split(\"_\")]\n",
    "    dobs, seed = int(dobs), int(seed)\n",
    "    row = [model, lmbda, rho, alpha, dobs, seed]\n",
    "    \n",
    "    # extract our predictions on the desired interval (t_max, t_max_pred]\n",
    "    preds = pd.read_csv(f\"results/pinn_forecasting/{fname}/oos_preds.csv\")\\\n",
    "    .query(f\"t > {t_max} and t <= {t_max_pred}\").set_index(\"t\")\n",
    "    preds.index = np.round(preds.index, 5)\n",
    "    \n",
    "    # get our truth\n",
    "    truth = pd.read_csv(f\"data/LORENZ_rho={rho}_alpha={alpha}_seed={seed}.csv\")\\\n",
    "    .query(f\"t > {t_max} and t <= {t_max_pred}\")[[\"t\", \"X_true\", \"Y_true\", \"Z_true\"]].set_index(\"t\")\n",
    "    truth.index = np.round(truth.index, 5); truth = truth.loc[preds.index]\n",
    "    \n",
    "    # compute our RMSEs on the forecasting regions only\n",
    "    X_rmse, Y_rmse, Z_rmse = np.sqrt(((truth.values - preds.values) ** 2).mean(axis=0))\n",
    "    \n",
    "    # add to our row, append to our dataframe\n",
    "    row += [X_rmse, Y_rmse, Z_rmse]\n",
    "    forecast_logs.loc[len(forecast_logs.index)] = row\n",
    "    \n",
    "# save our dataframe to .csv\n",
    "forecast_logs.to_csv(\"logs/forecast_logs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce05f341",
   "metadata": {},
   "source": [
    "# Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce651214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model governing the Lorenz system, appropriate for tensorflow vectorization\n",
    "def f_vec(t, X, thetas):\n",
    "    '''\n",
    "    1. X - array containing (X, Y, Z) components. Suppose it is (N x D) for vectorization.\n",
    "    2. theta - array containing (beta, rho, sigma) components.\n",
    "    '''\n",
    "    return tf.concat([thetas[2] * (X[:,1:2] - X[:,0:1]), # dx/dt = sigma * (y-x)\n",
    "                      X[:,0:1] * (thetas[1] - X[:,2:3]) - X[:,1:2], # dy/dt = x * (rho - z) - y\n",
    "                      X[:,0:1]*X[:,1:2] - thetas[0]*X[:,2:3], # dz/dt = x*y - beta*z\n",
    "                     ], axis=1)\n",
    "\n",
    "# fix our t_max\n",
    "t_max = 8.0\n",
    "\n",
    "# create a dataframe to store all of our physics residuals\n",
    "columns = [\"model\", \"lmbda\", \"rho\", \"alpha\", \"dobs\", \"seed\", \"X_pRMSE\", \"Y_pRMSE\", \"Z_pRMSE\"]\n",
    "physics_logs = pd.DataFrame(data=None, columns=columns)\n",
    "\n",
    "# computing physics residuals - let's do separate figures for each seed\n",
    "for rho in [23.0, 28.0]:\n",
    "    for alpha in [0.05]:\n",
    "        for dobs in [10]:\n",
    "            \n",
    "            # get the discretization\n",
    "            discret = int(np.log2(40 // dobs))\n",
    "            \n",
    "            # separate figures for each seed!\n",
    "            for seed in tqdm(range(100)):\n",
    "                \n",
    "                # load in our data\n",
    "                raw_data = pd.read_csv(f\"data/LORENZ_rho={rho}_alpha={alpha}_seed={seed}.csv\").query(f\"t <= {t_max}\")\n",
    "                obs_data = raw_data.iloc[::int((raw_data.index.shape[0] - 1) / (dobs * t_max))]\n",
    "                ts_obs = obs_data.t.values.astype(np.float64)\n",
    "                X_obs = obs_data[[\"X_obs\", \"Y_obs\", \"Z_obs\"]].to_numpy().astype(np.float64)\n",
    "                \n",
    "                ######## START WITH MAGI FIRST! ########\n",
    "                \n",
    "                # create our MAGI model + \"discretize\" our data\n",
    "                model = magi_v2.MAGI_v2(D_thetas=3, ts_obs=ts_obs, X_obs=X_obs, bandsize=None, f_vec=f_vec)\n",
    "                I, X_obs_discret = model._discretize(model.ts_obs, model.X_obs, discret)\n",
    "                X_interp_obs = model._linear_interpolate(X_obs_discret)\n",
    "                model.mu_ds = X_interp_obs.mean(axis=0)\n",
    "                \n",
    "                # load in our MAGI results\n",
    "                fname = f\"results/magi/rho={rho}_alpha={alpha}_dobs={dobs}_discret={discret}_seed={seed}.pickle\"\n",
    "                with open(fname, \"rb\") as file:\n",
    "                    results = pickle.load(file)\n",
    "                    X_samp, theta_samp = results[\"X_samps\"], results[\"thetas_samps\"]\n",
    "                    \n",
    "                # update our phi matrices\n",
    "                model.update_kernel_matrices(\n",
    "                    I_new=results[\"I\"], \n",
    "                    phi1s_new=results[\"phi1s\"], \n",
    "                    phi2s_new=results[\"phi2s\"]\n",
    "                )\n",
    "                \n",
    "                #### WORKING ON THE MEAN ####\n",
    "                \n",
    "                # now, compute the derivatives using the mean-X/theta\n",
    "                X_mean, theta_mean = X_samp.mean(axis=0)[None,:], theta_samp.mean(axis=0)[None,:]\n",
    "                X_cent_mean = tf.reshape(\n",
    "                    X_mean - model.mu_ds, \n",
    "                    shape=(X_mean.shape[0], X_mean.shape[1], 1, X_mean.shape[2])\n",
    "                )\n",
    "                f_gp_magi_mean = model.m_ds @ tf.transpose(X_cent_mean, perm=[0, 3, 1, 2])\n",
    "                f_gp_magi_mean = tf.transpose(f_gp_magi_mean, perm=[0, 2, 1, 3])[:,:,:,0].numpy()\n",
    "                \n",
    "                # compute the ODE derivatives on the MAGI samples\n",
    "                dXdt_ode_magi_mean = theta_mean[:,None,2:3] * (X_mean[:,:,1:2] - X_mean[:,:,0:1])\n",
    "                dYdt_ode_magi_mean = (X_mean[:,:,0:1] * (theta_mean[:,None,1:2] - X_mean[:,:,2:3])) - X_mean[:,:,1:2]\n",
    "                dZdt_ode_magi_mean = (X_mean[:,:,0:1] * X_mean[:,:,1:2]) - (theta_mean[:,None,0:1] * X_mean[:,:,2:3])\n",
    "                f_ode_magi_mean = np.concatenate(\n",
    "                    [dXdt_ode_magi_mean, \n",
    "                     dYdt_ode_magi_mean, \n",
    "                     dZdt_ode_magi_mean\n",
    "                    ], axis=2)\n",
    "                \n",
    "                # get the residuals + take their mean\n",
    "                magi_physics_residuals_mean = np.sqrt(((f_gp_magi_mean - f_ode_magi_mean)[0] ** 2).mean(axis=0))\n",
    "                \n",
    "                # add to our row\n",
    "                row = [\"magi\", np.nan, rho, alpha, dobs, seed] + list(magi_physics_residuals_mean)\n",
    "                physics_logs.loc[len(physics_logs.index)] = row\n",
    "                \n",
    "                # now go thru all the PINNs\n",
    "                for i, lmbda in enumerate([0.1, 1.0, 10.0, 100.0, 1000.0]):\n",
    "\n",
    "                    # load in the physics residuals for this neural network\n",
    "                    physics_residuals = pd.read_csv(f\"results/pinn/lmbda={lmbda}_rho={rho}_\" + \\\n",
    "                                                    f\"alpha={alpha}_dobs={dobs}_seed={seed}/physics_residuals.csv\")\n",
    "                    physics_residuals = physics_residuals[[\"X\", \"Y\", \"Z\"]].to_numpy()\n",
    "                    physics_residuals_pinn = np.sqrt((physics_residuals ** 2).mean(axis=0))\n",
    "\n",
    "                    # add to our row\n",
    "                    row = [\"pinn\", lmbda, rho, alpha, dobs, seed] + list(physics_residuals_pinn)\n",
    "                    physics_logs.loc[len(physics_logs.index)] = row\n",
    "                \n",
    "# save our physics logs\n",
    "physics_logs.to_csv(\"logs/physics_logs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Afterburner)",
   "language": "python",
   "name": "afterburner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
